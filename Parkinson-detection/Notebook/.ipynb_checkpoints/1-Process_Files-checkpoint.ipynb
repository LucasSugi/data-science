{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Files\n",
    "\n",
    "Este notebook é responsável por realizar pré-processamento e limpeza nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Read data about control group'''\n",
    "def read_control():\n",
    "    #Path to data\n",
    "    data_path = os.path.dirname(os.getcwd()) + '/Data/handwrite' + '/control/'\n",
    "\n",
    "    #List files in data_patabsh\n",
    "    filenames = os.listdir(data_path)\n",
    "\n",
    "    #Dataframe with all control's people\n",
    "    df_control = pd.DataFrame()\n",
    "\n",
    "    for file in filenames:\n",
    "        #Read data\n",
    "        df = pd.read_csv(data_path + file,sep=';',header=None,names=['X','Y','Z','pressure','gripAngle','timestamp','test_id'])\n",
    "\n",
    "        #Set an id \n",
    "        df['id'] = file.split('.')[0]\n",
    "\n",
    "        #Set that the person does not have parkinson\n",
    "        df['parkinson'] = False\n",
    "\n",
    "        #Concat results\n",
    "        df_control = pd.concat([df_control,df],axis=0)\n",
    "        \n",
    "    return df_control\n",
    "        \n",
    "'''Read data about parkinson group'''\n",
    "def read_parkinson():\n",
    "    #Path to data\n",
    "    data_path = os.path.dirname(os.getcwd()) + '/Data/handwrite' + '/parkinson/'\n",
    "\n",
    "    #List files in data_patabsh\n",
    "    filenames = os.listdir(data_path)\n",
    "\n",
    "    #Dataframe with all control's people\n",
    "    df_parkinson = pd.DataFrame()\n",
    "\n",
    "    for file in filenames:\n",
    "        #Read data\n",
    "        df = pd.read_csv(data_path + file,sep=';',header=None,names=['X','Y','Z','pressure','gripAngle','timestamp','test_id'])\n",
    "\n",
    "        #Set an id \n",
    "        df['id'] = file.split('.')[0]\n",
    "\n",
    "        #Set that the person does not have parkinson\n",
    "        df['parkinson'] = True\n",
    "\n",
    "        #Concat results\n",
    "        df_parkinson = pd.concat([df_parkinson,df],axis=0)\n",
    "    \n",
    "    return df_parkinson\n",
    "\n",
    "'''Process files from dataset handwrite'''\n",
    "def process_hw():\n",
    "    #Read two datasets\n",
    "    df_control = read_control()\n",
    "    df_parkinson = read_parkinson()\n",
    "\n",
    "    #Concat two datasets\n",
    "    df_parkinson = pd.concat([df_parkinson,df_control])\n",
    "\n",
    "    #Select just the columns that are necessary\n",
    "    df_parkinson = df_parkinson.loc[:,['id','test_id','X','Y','timestamp','parkinson']]\n",
    "\n",
    "    #Sort values\n",
    "    df_parkinson.sort_values(['id','test_id','timestamp'],inplace=True)\n",
    "\n",
    "    #Save result\n",
    "    df_parkinson.to_csv(os.path.dirname(os.getcwd()) + '/Data/handwrite/' + 'parkinson_hw.csv',index=False)\n",
    "    \n",
    "\n",
    "'''Process each user's file'''\n",
    "def process_user():\n",
    "    #Path to user's files\n",
    "    path = os.path.dirname(os.getcwd()) + '/Data/tappy-keystroke' + '/Archived users/'\n",
    "\n",
    "    #List files in path\n",
    "    filename =  os.listdir(path)\n",
    "    filename.sort()\n",
    "\n",
    "    #Create dataframe to save information\n",
    "    df_parkinson = pd.DataFrame(columns=['birthyear','gender','parkinson','tremor','diagnosisYear','sided','UPDRS','impact','levadopa','DA','MAOB','other'])\n",
    "\n",
    "    #Identifier of user\n",
    "    id_user = []\n",
    "\n",
    "    for file in filename:\n",
    "\n",
    "        #Read file\n",
    "        with open(path + file) as f:\n",
    "            user = f.readlines()\n",
    "\n",
    "        #Extract information\n",
    "        user = list(map(lambda x: x.split(': ')[1].replace('\\n',''),user))\n",
    "        id_user.append(file.split('_')[1].split('.')[0])\n",
    "\n",
    "        #Fix dataframe\n",
    "        user = pd.DataFrame(user).transpose()\n",
    "        user.columns = df_parkinson.columns\n",
    "\n",
    "        #Concat result\n",
    "        df_parkinson = pd.concat([df_parkinson,user],axis=0)\n",
    "\n",
    "    #ID like index\n",
    "    df_parkinson['id'] = id_user\n",
    "    df_parkinson.set_index('id',inplace=True)\n",
    "\n",
    "    #Save data\n",
    "    df_parkinson.to_csv(os.path.dirname(os.getcwd()) + '/Data/tappy-keystroke/user_information.csv')\n",
    "    \n",
    "'''Process each tappy's file'''\n",
    "def process_tappy():\n",
    "    #Path to tappy data's files\n",
    "    path = os.path.dirname(os.getcwd()) + '/Data/tappy-keystroke' + '/Tappy Data/'\n",
    "\n",
    "    #List files in path\n",
    "    filename =  os.listdir(path)\n",
    "    filename.sort()\n",
    "\n",
    "    #Create dataframe with result\n",
    "    df_parkinson = pd.DataFrame(columns=['id','date','timestamp','hand','hold_time','direction','latency','flight'])\n",
    "\n",
    "    for index,file in enumerate(filename):\n",
    "        print(index)\n",
    "\n",
    "        try:\n",
    "            #Read data\n",
    "            df_tmp = pd.read_csv(path+file,sep='\\t',usecols=[0,1,2,3,4,5,6,7],low_memory=False)    \n",
    "            df_tmp.columns = df_parkinson.columns\n",
    "\n",
    "            #Concat result\n",
    "            df_parkinson = pd.concat([df_parkinson,df_tmp],axis=0)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    #ID like index\n",
    "    df_parkinson.set_index('id',inplace=True)\n",
    "        \n",
    "    #Save data\n",
    "    df_parkinson.to_csv(os.path.dirname(os.getcwd()) + '/Data/tappy-keystroke/tappy_data.csv')\n",
    "    \n",
    "'''Perform a clean in dataset tappy'''\n",
    "def clean_tappy():\n",
    "    \n",
    "    #Path to data\n",
    "    data_path = os.path.dirname(os.getcwd()) + '/Data/tappy-keystroke/'\n",
    "    \n",
    "    #Read dataset\n",
    "    df_tappy = pd.read_csv(data_path + 'tappy_data.csv',usecols=[0,3,4,5,6,7])\n",
    "\n",
    "    #Filter rows with problem\n",
    "    df_tappy = df_tappy.loc[df_tappy['hand'].isin(['L','R','S'])]\n",
    "    df_tappy = df_tappy.loc[df_tappy['direction'].isin(['LL', 'LS', 'SL', 'LR', 'RR', 'RL', 'RS', 'SR', 'SS'])]\n",
    "\n",
    "    #Create regular expression\n",
    "    prog =re.compile('^[0-9]+\\.[0-9]*$')\n",
    "\n",
    "    #Fix columns\n",
    "    df_tappy['latency'] = df_tappy['latency'].map(lambda x: x if type(x) == float else (float(x) if prog.match(x) is not None else np.nan))\n",
    "    df_tappy['flight'] = df_tappy['flight'].map(lambda x: x if type(x) == float else (float(x) if prog.match(x) is not None else np.nan))\n",
    "\n",
    "    #Change type\n",
    "    df_tappy.loc[:,'hold_time'] = df_tappy.loc[:,'hold_time'].astype(float)\n",
    "\n",
    "    #Exclude negative numbers in hold_time\n",
    "    df_tappy = df_tappy[df_tappy['hold_time'] > 0]\n",
    "    \n",
    "    #Filter data to exclude incorrect information\n",
    "    df_tappy = df_tappy.loc[(df_tappy['latency']-df_tappy['hold_time']>0),:]\n",
    "    df_tappy = df_tappy.loc[df_tappy['latency'] < 2000,:]\n",
    "\n",
    "    #Have one row in dataset that doesn't have id but we infer that is QEYMRM1ZSM because of sequence\n",
    "    df_tappy['id'] = df_tappy['id'].fillna('QEYMRM1ZSM')\n",
    "\n",
    "    #Save new data\n",
    "    df_tappy.to_csv(data_path + 'tappy_data_cleaned.csv',index=False)\n",
    "    \n",
    "'''Perform clean in dataset user information'''    \n",
    "def clean_user():\n",
    "    \n",
    "    #Path to data\n",
    "    data_path = os.path.dirname(os.getcwd()) + '/Data/tappy-keystroke/'\n",
    "    \n",
    "    #Read data\n",
    "    df_user = pd.read_csv(data_path + 'user_information.csv',usecols=[0,3])\n",
    "    \n",
    "    #Excludes people who do not have reliable information\n",
    "    df_user = df_user.loc[~df_user['id'].isin(['LA6KW35OXK','UH6FQWXIZI','VCFUOTMSKT']),:]\n",
    "\n",
    "    #Save result\n",
    "    df_user.to_csv(data_path + 'user_information_cleaned.csv',index=False)\n",
    "    \n",
    "'''Compute new metrics for col_pivot'''\n",
    "def metric(df,col_pivot):\n",
    "    \n",
    "    #Creation of new metrics\n",
    "    df_metrics = df.groupby(['id',col_pivot]).apply(lambda x: x.loc[:,['hold_time','latency','flight']].agg([np.mean,np.std]))\n",
    "    df_metrics.reset_index(level=[1,2],inplace=True)\n",
    "    df_metrics.rename(columns={'level_2':'metric'},inplace=True)\n",
    "\n",
    "    #Apply pivot in table to get new columns\n",
    "    df_metrics = pd.pivot_table(df_metrics,values=['hold_time','latency','flight'],columns=[col_pivot,'metric'],index=['id'])\n",
    "\n",
    "    #Fix columns\n",
    "    cols = []\n",
    "    for col in df_metrics.columns:\n",
    "        cols.append(col[0] + '_' + col[1] + '_' + col[2])\n",
    "    df_metrics.columns = cols\n",
    "    df_metrics.reset_index(inplace=True)\n",
    "    \n",
    "    return df_metrics\n",
    "\n",
    "#Fill na in dataframe with mean\n",
    "def fill_na(df):\n",
    "    return df.fillna(df.iloc[:,2:].mean())\n",
    "\n",
    "'''Perform merge in two datasets'''\n",
    "def merge_data(col_pivot):\n",
    "    \n",
    "    #Path to data\n",
    "    data_path = os.path.dirname(os.getcwd()) + '/Data/tappy-keystroke/'\n",
    "    \n",
    "    #Read data\n",
    "    df_user = pd.read_csv(data_path+'user_information_cleaned.csv')\n",
    "    df_tappy = pd.read_csv(data_path+'tappy_data_cleaned.csv')\n",
    "\n",
    "    #Compute new metrics\n",
    "    df_metrics = metric(df_tappy,col_pivot)\n",
    "\n",
    "    #Merge two files\n",
    "    df_merge = pd.merge(df_user,df_metrics,on='id')\n",
    "    \n",
    "    #Fill na values\n",
    "    df_merge = df_merge.groupby('parkinson').apply(fill_na)\n",
    "\n",
    "    #Fix somethings\n",
    "    del df_merge['parkinson']\n",
    "    df_merge = df_merge.reset_index(level=0).set_index('id')\n",
    "\n",
    "    #Save result\n",
    "    df_merge.to_csv(data_path+'parkinson_tappy_' + col_pivot + '.csv')\n",
    "    \n",
    "'''Criation of new metrics for dataset handwrite'''\n",
    "def new_metric_hw():\n",
    "    #Path to data\n",
    "    data_path = os.path.dirname(os.getcwd()) + '/Data/'\n",
    "\n",
    "    #Read data\n",
    "    df_parkinson = pd.read_csv(data_path + '/handwrite/parkinson_hw.csv')\n",
    "\n",
    "    #We drop equal positions because we want to know just the changes along time\n",
    "    df_parkinson = df_parkinson.drop_duplicates(['id','test_id','X','Y'],keep='first')\n",
    "\n",
    "    #Compute velocity for each id and test\n",
    "    df_velocity = df_parkinson.groupby(['id','test_id']).apply(lambda x: (np.sqrt(x['X'].diff()**2 + x['Y'].diff()**2) / x['timestamp'].diff()).agg([np.mean,np.std]))\n",
    "\n",
    "    #Fix dataframe\n",
    "    df_velocity.reset_index(inplace=True)\n",
    "    df_velocity.dropna(inplace=True)\n",
    "\n",
    "    #Insert column that indicates if person has or not parkinson\n",
    "    map_parkinson = df_parkinson.set_index('id')['parkinson'].to_dict()\n",
    "    df_velocity['parkinson'] = df_velocity['id'].map(map_parkinson)\n",
    "\n",
    "    #Save data\n",
    "    df_velocity.to_csv(data_path + '/handwrite/parkinson_hw_velocity.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n"
     ]
    }
   ],
   "source": [
    "#Process dataset\n",
    "process_hw()\n",
    "process_user()\n",
    "process_tappy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções acima transformam os dados no formato csv para facilitar a manipulação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sugi/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3191: DtypeWarning: Columns (4,6,7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if (yield from self.run_code(code, result)):\n"
     ]
    }
   ],
   "source": [
    "#Clean two files\n",
    "clean_user()\n",
    "clean_tappy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As funções acima realizam as seguintes atividades de limpeza:\n",
    "\n",
    "- tappy:\n",
    "    - Arquivo bruto possui linhas concatenadas (provalmente falha do sistema na gravação) causando falha na leitura das colunas. A solução foi remover essas linhas dada a dificuldade de tratá-las.\n",
    "    - Exclusão de linhas com informações inconsistentes com a fórmula latency-hold_time > 0.\n",
    "- user_information:\n",
    "    - Exclusão de colunas: Excluímos informações que não relacionadas ao teclado por nosso objetivo ser utilizar apenas os dados de tecla para predição.\n",
    "    - Usuários com informações estranhas: Existem pessoas que afirmam não possuir parkinson apesar de colocar um ano como diagnóstico. Como não é possível saber a veracidade disso nós removemos esses usuários."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge files and creation of new metrics\n",
    "merge_data('hand')\n",
    "merge_data('direction')\n",
    "new_metric_hw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por alguma razão os dois datasets tappy-keystroke não possuem identificadores iguais, por causa disso nós verificamos quais seriam os valores similares em ambos realizando um merge.\n",
    "\n",
    "Além disso, na função acima também criamos novas métricas que são resumos (médias) dos atributos numéricos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
